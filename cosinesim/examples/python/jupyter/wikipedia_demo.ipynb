{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec5ee12",
   "metadata": {},
   "source": [
    "# Welcome to Xilinx Cosine Similarity Acceleration Demo \n",
    "---\n",
    "\n",
    "**This Notebook demonstrates how to use the Xilinx Cosine Similarity product and shows the power of Xilinx FPGAs to accelerate Cosine Similarity**\n",
    "\n",
    "---\n",
    "\n",
    "### The Demo : Wiki Search Engine \n",
    "\n",
    "In this Demo Example, we will create Search Engine based on Wikipedia Data. \n",
    "\n",
    "The User will provide <u>Keyword</u> ( or ) <u>Context Phrase</u> to search for the related information.\n",
    "\n",
    "This Example will take the given Keyword / Phrase and filter out the Wikipedia Documents and returns the Top Matching Information. \n",
    "\n",
    "The Top Matching are calculated based on similarity between the given Keyword and all Wikipedia Pages. This similarity is know as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "Instead of finding Similarity with the direct one-hot word representation, we used [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) Word Embeddings, which maps words into more meaningful space.\n",
    "\n",
    "In General, finding Cosine Similarity on large dataset will take a huge amount of time on CPU \n",
    "\n",
    "With the Xilinx Cosine Similarity Acceleration, it will speedup the process by > ~ 80x\n",
    "\n",
    "We will use the Xilinx Cosine Similarity module (**xilCosineSim**) and setup a population against which similarity of target vectors can be calculated.\n",
    "\n",
    " \n",
    "### The Demo is Structured in Six Sections :\n",
    "1. [**Download Wikipedia Data & GloVe Embeddings File**](#DownloadFiles)\n",
    "<br><br>\n",
    "2. [**Load and Parse Wikipedia XML File**](#LoadandParse)\n",
    "<br><br>\n",
    "3. [**Clean the XML Data**](#DataClean)\n",
    "<br><br>\n",
    "4. [**Calculate the Embeddings Representation for All Wiki Pages**](#GloVe)\n",
    "<br><br>\n",
    "5. [**Load the Embeddings representation of Wiki Pages into U50 HBM Memory**](#ConfigureDevice)\n",
    "<br><br>\n",
    "6. [**Run Cosine Similarity to Find out the TopK Matchings for the given Query**](#TopKMatchings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1475de4",
   "metadata": {},
   "source": [
    " #### Load Xilinx Cosine Similarity Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xilCosineSim as xcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca1383",
   "metadata": {},
   "source": [
    "#### Load Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76fbce",
   "metadata": {},
   "source": [
    "#### Download the Wikipedia Data Dump <a id=\"DownloadFiles\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\") :\n",
    "    print(\"Downloading Wikipedia File ...\")\n",
    "    os.system(\"wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\")\n",
    "    print(\"Download Completed !!\")\n",
    "if not os.path.isfile(\"enwiki-latest-pages-articles-multistream1.xml-p1p41242\") :\n",
    "    os.system(\"bzip2 -d enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd36393",
   "metadata": {},
   "source": [
    "#### Download the GloVe File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68161958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"glove.6B.50d.txt.tar\") :\n",
    "    print(\"Downloading GloVe Embedding File ...\")\n",
    "    os.system(\"wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ogyMmAu0fcZBdSwTQJuX6jHLzlTJnql0' -O glove.6B.50d.txt.tar\")\n",
    "    print(\"Download Completed !!\")\n",
    "if not os.path.isfile(\"glove.6B.50d.txt\") :\n",
    "    os.system(\"tar -xvzf glove.6B.50d.txt.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd001d2f",
   "metadata": {},
   "source": [
    "#### Parsing Load and Parse the Wikipedia XML File <a id=\"LoadandParse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse_Wikipedia_XML(wikipedia_xml, Max_Pages=None):\n",
    "    \n",
    "    '''\n",
    "    The Input is a wikipedia xml compressed (whole/partial) file. \n",
    "    This file can be downloaded from https://dumps.wikimedia.org/enwiki/ \n",
    "    To decompress, Use : bzip2 -d <bz2_compressed_xml_file_path>\n",
    "    \n",
    "    Parse the XML file from root. This defination parse the children of roots and go through its modules. \n",
    "    Every child with a 'page' as attribute is a single wikipedia page. \n",
    "    The XML file is consisted with multiple such pages. \n",
    "    In each page, it contains title, text and other metadata items. \n",
    "    We make a dictionary with title as key & text as value.\n",
    "    '''\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    tree = ET.parse(wikipedia_xml)\n",
    "    print(f'Wikipedia XML file Load completed in  : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    tStart = time.perf_counter()\n",
    "    root = tree.getroot()\n",
    "    dictionary = {}\n",
    "    if Max_Pages != None : \n",
    "        root = root[0:Max_Pages]\n",
    "    for child in root :\n",
    "        if \"page\" in child.tag : \n",
    "            for branch in child:\n",
    "                if \"title\" in branch.tag:\n",
    "                    if branch.text.isupper() :\n",
    "                        title = branch.text\n",
    "                    else : \n",
    "                        title_list = re.findall('[A-Z][a-z]*', branch.text)\n",
    "                        title = \" \".join(title_list)\n",
    "                    dictionary[title] = \"\"\n",
    "                if \"redirect\" in branch.tag :\n",
    "                    dictionary.pop(title)\n",
    "                if \"revision\" in branch.tag : \n",
    "                    for chunk in branch:\n",
    "                        if \"text\" in chunk.tag:\n",
    "                            number = 0\n",
    "                            for line in chunk.text.split(\"\\n\") : \n",
    "                                if \"|\" not in line[0:5] and \"{{\" not in line[0:5] and \"}}\" not in line[0:5]:\n",
    "                                    if len(line) > 100 : \n",
    "                                        number = number + 1\n",
    "                                        try:\n",
    "                                            dictionary[title] = dictionary[title] + \" \" + line\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                        if number > 5: # Only Read First Five Paragraphs \n",
    "                                            break\n",
    "    print(f'Wikipedia XML Data Parse completed in : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639690a4",
   "metadata": {},
   "source": [
    "#### Helper Functions to Clean the Data <a id=\"DataClean\"></a>\n",
    "---\n",
    "###### 1. Remove words which does not contribute to context \n",
    "###### 2. Remove Punctuation \n",
    "###### 3. Remove Extra Spaces <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9efb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\\\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\\\n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\\\n",
    "             \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \\\n",
    "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \\\n",
    "             \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \\\n",
    "             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\\\n",
    "             \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\\\n",
    "             \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \\\n",
    "             \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "def Remove_Stopwords(text):\n",
    "    return \" \".join([item for item in text.split(\" \") if item not in stopwords])\n",
    "\n",
    "def Remove_Punctuation(text):\n",
    "    new_text = \"\"\n",
    "    for char in text: \n",
    "        if char in string.punctuation:\n",
    "            new_text = new_text + \" \"\n",
    "        else : \n",
    "            new_text = new_text + char\n",
    "    return new_text  \n",
    "\n",
    "def Remove_Extraspaces(text):\n",
    "    return \" \".join([ item for item in text.split() if item])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874e3b8",
   "metadata": {},
   "source": [
    "#### Defination to Apply the Cleaning Methods on Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97713c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apply_Cleaning(data_frame):\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    data_frame[\"Lower\"] = data_frame[\"Text\"].apply(lambda x: x.lower())\n",
    "    data_frame[\"RemovePunctuation\"] = data_frame[\"Lower\"].apply(lambda x: Remove_Punctuation(x))\n",
    "    data_frame[\"RemoveExtraSpaces\"] = data_frame[\"RemovePunctuation\"].apply(lambda x: Remove_Extraspaces(x))\n",
    "    data_frame[\"RemoveStopWords\"] = data_frame[\"RemoveExtraSpaces\"].apply(lambda x: Remove_Stopwords(x))  \n",
    "    print(f'Data Cleaning Completed in {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa76643",
   "metadata": {},
   "source": [
    "#### Load the GloVe File & Create and Accessible Lookup Dictionary <a id=\"GloVe\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de788eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Glove(glove_file_path):\n",
    "    \n",
    "    '''\n",
    "    The GloVe file contains Embeddings for 400,000 words.\n",
    "    In each line, the first item is a 'word' representation. \n",
    "    And rest of the values in the line, seperated by spaces are it's 50 Embedding values.  \n",
    "    Here we creating dictionary, with each word as a key and it's 50 Embedding representation as value. \n",
    "    '''\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    glove_file = open(glove_file_path, encoding=\"utf8\")\n",
    "    for line in glove_file.readlines():\n",
    "        line_list = line.split(\" \")\n",
    "        temp_list = line_list[1:-1]\n",
    "        temp_list.append(line_list[-1].split(\"\\n\")[0])\n",
    "        float_vector = [vector for vector in np.array(temp_list, dtype=np.float32)]\n",
    "        glove_dict[line_list[0]] = float_vector\n",
    "    glove_file.close()\n",
    "    print(f'Loading GloVe Embedding File completed in : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    print(f'Number of words in vocabulary, having GloVe representation : {len(glove_dict)}')\n",
    "    return glove_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d570a",
   "metadata": {},
   "source": [
    "#### Map GloVe Vector for each Word in Sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c901f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glove_Formatting(x):\n",
    "    \n",
    "    '''\n",
    "    Given a sentence, map each word to it's 50 dimenssional embeddings. \n",
    "    If the word is not found in the GloVe dictionary, create 50 Dimenssional vector with all zeros. \n",
    "    '''\n",
    "    \n",
    "    glove_vector = []\n",
    "    for item in x:\n",
    "        try:\n",
    "            glove_vector.append(glove_dict[item])\n",
    "        except:\n",
    "            glove_vector.append([0.0 for i in range(50)])\n",
    "    return np.array(glove_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c312b74",
   "metadata": {},
   "source": [
    "#### Map Embeddings of each Word and find the Average of Embeddings for Senetence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map_GloVe_Embeddings(data_frame):\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    data_frame[\"Embeddings\"] = data_frame[\"RemoveStopWords\"].apply(lambda x : Glove_Formatting(x.split(\" \")) )\n",
    "    data_frame[\"EmbeddingsAverage\"] = data_frame[\"Embeddings\"].apply(lambda x : np.sum(x, axis=0)/len(x))\n",
    "    print(f'Embeddings Mapping for the entire Wiki data, Completed in {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46612621",
   "metadata": {},
   "source": [
    "#### Provide Wikipedia XML File Path & Call Parsing Function</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_xml = \"enwiki-latest-pages-articles-multistream1.xml-p1p41242\"\n",
    "dictionary = Parse_Wikipedia_XML(wikipedia_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0006a06",
   "metadata": {},
   "source": [
    "#### Load GloVe File & Create GloVe Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = \"glove.6B.50d.txt\"\n",
    "glove_dict = {}\n",
    "glove_dict = Load_Glove(glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117d579",
   "metadata": {},
   "source": [
    "#### Creating Pandas Data Frame from the XML with Title and Text as Columns & each Row is an entry of each Wiki Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "value_list = []\n",
    "data_frame = pd.DataFrame()\n",
    "for key, value in dictionary.items():\n",
    "    key_list.append(key) \n",
    "    value_list.append(value)\n",
    "data_frame[\"Title\"] = key_list\n",
    "data_frame[\"Text\"]  = value_list\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1733f",
   "metadata": {},
   "source": [
    "#### Apply Cleaning for Dataframe & derive Embeddings Average for Wiki Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = Apply_Cleaning(data_frame)\n",
    "data_frame = Map_GloVe_Embeddings(data_frame)\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2430b",
   "metadata": {},
   "source": [
    "#### Assertion Check for the size of final Average Embedding Dimenssion <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_frame)):\n",
    "    assert data_frame[\"EmbeddingsAverage\"][i].shape[0] ==  50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd5c83",
   "metadata": {},
   "source": [
    "#### Input the Embeddings Vector size, Length of the Population & Datatype to configure Load for FPGA  <a id=\"ConfigureDevice\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0355a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorLength = 50\n",
    "NumVectors = len(data_frame)\n",
    "Bytes_Per_value = 4\n",
    "NumDevices = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3188f",
   "metadata": {},
   "source": [
    "####  Configure Population Load in FPGA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = xcs.options()\n",
    "opt.vecLength = VectorLength\n",
    "opt.numDevices = NumDevices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbcc4a",
   "metadata": {},
   "source": [
    "#### U50 having 8GB of HBM Memory. Check if the given Data Load has exceeded the Limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47967ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert VectorLength * NumVectors * Bytes_Per_value  <  NumDevices * 8 * 2**30, \"Memory in 1 x U50 is 8GB. Cant Load the Given amount of data into Memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7b72b",
   "metadata": {},
   "source": [
    "#### Load the Population Embeddings into U50 HBM Memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = xcs.cosinesim(opt, Bytes_Per_value)\n",
    "cs.startLoadPopulation(NumVectors)\n",
    "for vecNum in range(NumVectors):\n",
    "    vecBuf = cs.getPopulationVectorBuffer(vecNum)\n",
    "\n",
    "    valVec = []\n",
    "    for vecIdx in range(VectorLength):\n",
    "        valVec.append((int(data_frame[\"EmbeddingsAverage\"][vecNum][vecIdx]*1000)))  # Converting Float32 Value to Int Type\n",
    "    vecBuf.append(valVec)\n",
    "    cs.finishCurrentPopulationVector(vecBuf)\n",
    "\n",
    "cs.finishLoadPopulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18e629",
   "metadata": {},
   "source": [
    "####  Find the TopK Matchings for the Given Query <a id=\"TopKMatchings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4be072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_TopK_Matchings(query, topK=10):\n",
    "    \n",
    "    '''\n",
    "    Apply the Cleaning, GloVe Mapping function on the given Query.\n",
    "    Call the Xilinx Cosine Simalrity Match Target Vector API to fiind the Top Matchings with the Loaded Population.\n",
    "    Displlay the Mathcing Pages Information.\n",
    "    '''\n",
    "    \n",
    "    query_clean = Remove_Stopwords(Remove_Extraspaces(Remove_Punctuation(query.lower())))\n",
    "    query_embedding = np.sum(Glove_Formatting(query_clean.split(\" \")), axis=0)/len(query_clean.split(\" \"))*1000\n",
    "    targetVec = query_embedding.astype(\"int32\")\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    result = cs.matchTargetVector(topK, targetVec)\n",
    "    print(f'completed in {1000*(time.perf_counter() - tStart):.6f} msec\\n')\n",
    "    print(\"RANK  ID    Wiki Title \\t\\t\\t\\t\\t\\t  MESSAGE \\t\\t\\t\\t        CONFIDENCE\")\n",
    "    print(\"----|-----|-------------|\" + 65 *\"-\" + \"---------------------|---------\")\n",
    "    num = 0\n",
    "    for item in result:\n",
    "        num = num +1\n",
    "        Message = data_frame[\"RemoveStopWords\"][item.index]\n",
    "        print(\"{:02d}\".format(num) + 3*\" \" + \"{:05d}\".format(item.index) + 3*\" \"+ \\\n",
    "              '{message: <10}'.format(message=Remove_Extraspaces(data_frame[\"Title\"][item.index][0:10])) + \\\n",
    "              3*\" \" + Message[0:35] + \" ... \" + Message[-45:-1] + 3* \" \" +\\\n",
    "              '{:.6f}'.format(item.similarity) )\n",
    "    print(f'\\nTopK Matchings completed in {1000*(time.perf_counter() - tStart):.6f} msec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f21306",
   "metadata": {},
   "source": [
    "---\n",
    "#### Upto this point, it is just a One Time Load & Execution. Now, once all the Data is Loaded, we can run any number of Queries \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577706e6",
   "metadata": {},
   "source": [
    "#### Call the TopK Matchings Function with Your Query Input\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d758a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Find_TopK_Matchings(query=\"Agriculture\", topK=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbd807",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Find_TopK_Matchings(query=\"Battle Ships\", topK=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861533d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Find_TopK_Matchings(query=\"Second World War\", topK=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Find_TopK_Matchings(query=\"Drug Discovery\", topK=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ecd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Find_TopK_Matchings(query=\"United Nations\", topK=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20374dcd",
   "metadata": {},
   "source": [
    "### <center> End of the Notebook </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
