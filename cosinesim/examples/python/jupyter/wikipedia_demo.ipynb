{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0ca1d0",
   "metadata": {},
   "source": [
    "# <font color='red'> Welcome to Xilinx Cosine Similarity Acceleration Demo </font>\n",
    "---\n",
    "**This Notebook demonstrates how to use the Xilinx Cosine Similarity product and shows the power of Xilinx FPGAs to accelerate Cosine Similarity**\n",
    "\n",
    "---\n",
    "\n",
    "### The Demo : <font color=\"red\" > Wiki Search Engine </font>\n",
    "\n",
    "In this Demo Example, we will create Search Engine based on Wikipedia Data. \n",
    "\n",
    "The User will provide <u>Keyword</u> ( or ) <u>Context Phrase</u> to search for the related information.\n",
    "\n",
    "This Example will take the given Keyword / Phrase and filter out the Wikipedia Documents and returns the Top Matching Information. \n",
    "\n",
    "The Top Matching are calculated based on similarity between the given Keyword and all Wikipedia Pages. This similarity is know as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "Instead of finding Similarity with the direct one-hot word representation, we used [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) Word Embeddings, which maps words into more meaningful space.\n",
    "\n",
    "In General, finding Cosine Similarity on large dataset will take a huge amount of time on CPU \n",
    "\n",
    "With the Xilinx Cosine Similarity Acceleration, it will speedup the process by > ~ 80x\n",
    "\n",
    "We will use the Xilinx Cosine Similarity module (**xilCosineSim**) and setup a population against which similarity of target vectors can be calculated.\n",
    "\n",
    " \n",
    "### The Demo is Structured in Six Sections :\n",
    "1. [**Download Wikipedia Data & GloVe Embeddings File**](#DownloadFiles)\n",
    "<br><br>\n",
    "2. [**Load and Parse Wikipedia XML File**](#LoadandParse)\n",
    "<br><br>\n",
    "3. [**Clean the XML Data**](#DataClean)\n",
    "<br><br>\n",
    "4. [**Calculate the Embeddings Representation for All Wiki Pages**](#GloVe)\n",
    "<br><br>\n",
    "5. [**Load the Embeddings representation of Wiki Pages into U50 HBM Memory**](#ConfigureDevice)\n",
    "<br><br>\n",
    "6. [**Run Cosine Similarity to Find out the TopK Matchings for the given Query**](#TopKMatchings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a91173",
   "metadata": {},
   "source": [
    " #### <font color='red'> Load Xilinx Cosine Similarity Library </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bff0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xilCosineSim as xcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb298a93",
   "metadata": {},
   "source": [
    "#### Load Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f276ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623d519",
   "metadata": {},
   "source": [
    "#### Download the Wikipedia Data Dump <a id=\"DownloadFiles\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05c1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dowloading Wikipedia File ...\n",
      "Download Completed !!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"enwiki-20210320-pages-articles-multistream1.xml-p1p41242.bz2\") :\n",
    "    print(\"Dowloading Wikipedia File ...\")\n",
    "    os.system(\"wget https://dumps.wikimedia.org/enwiki/20210320/enwiki-20210320-pages-articles-multistream1.xml-p1p41242.bz2\")\n",
    "    print(\"Download Completed !!\")\n",
    "if not os.path.isfile(\"enwiki-20210320-pages-articles-multistream1.xml-p1p41242\") :\n",
    "    os.system(\"bzip2 -d enwiki-20210320-pages-articles-multistream1.xml-p1p41242.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55d87d",
   "metadata": {},
   "source": [
    "#### Download the GloVe File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0a6846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dowloading GloVe Embedding File ...\n",
      "Download Completed !!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"glove.6B.50d.txt.tar\") :\n",
    "    print(\"Dowloading GloVe Embedding File ...\")\n",
    "    os.system(\"wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ogyMmAu0fcZBdSwTQJuX6jHLzlTJnql0' -O glove.6B.50d.txt.tar\")\n",
    "    print(\"Download Completed !!\")\n",
    "if not os.path.isfile(\"glove.6B.50d.txt\") :\n",
    "    os.system(\"tar -xvzf glove.6B.50d.txt.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4387136",
   "metadata": {},
   "source": [
    "#### Parsing Load and Parse the Wikipedia XML File <a id=\"LoadandParse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238b36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse_Wikipedia_XML(wikipedia_xml, Max_Pages=None):\n",
    "    \n",
    "    '''\n",
    "    The Input is a wikipedia xml compressed (whole/partial) file. \n",
    "    This file can be downloaded from https://dumps.wikimedia.org/enwiki/ \n",
    "    To decompress, Use : bzip2 -d <bz2_compressed_xml_file_path>\n",
    "    \n",
    "    Parse the XML file from root. This defination parse the children of roots and go through its modules. \n",
    "    Every child with a 'page' as attribute is a single wikipedia page. \n",
    "    The XML file is consisted with multiple such pages. \n",
    "    In each page, it contains title, text and other metadata items. \n",
    "    We make a dictionary with title as key & text as value.\n",
    "    '''\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    tree = ET.parse(wikipedia_xml)\n",
    "    print(f'Wikipedia XML file Load completed in  : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    tStart = time.perf_counter()\n",
    "    root = tree.getroot()\n",
    "    dictionary = {}\n",
    "    if Max_Pages != None : \n",
    "        root = root[0:Max_Pages]\n",
    "    for child in root :\n",
    "        if \"page\" in child.tag : \n",
    "            for branch in child:\n",
    "                if \"title\" in branch.tag:\n",
    "                    if branch.text.isupper() :\n",
    "                        title = branch.text\n",
    "                    else : \n",
    "                        title_list = re.findall('[A-Z][a-z]*', branch.text)\n",
    "                        title = \" \".join(title_list)\n",
    "                    dictionary[title] = \"\"\n",
    "                if \"redirect\" in branch.tag :\n",
    "                    dictionary.pop(title)\n",
    "                if \"revision\" in branch.tag : \n",
    "                    for chunk in branch:\n",
    "                        if \"text\" in chunk.tag:\n",
    "                            number = 0\n",
    "                            for line in chunk.text.split(\"\\n\") : \n",
    "                                if \"|\" not in line[0:5] and \"{{\" not in line[0:5] and \"}}\" not in line[0:5]:\n",
    "                                    if len(line) > 100 : \n",
    "                                        number = number + 1\n",
    "                                        try:\n",
    "                                            dictionary[title] = dictionary[title] + \" \" + line\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                        if number > 5: # Only Read First Five Paragraphs \n",
    "                                            break\n",
    "    print(f'Wikipedia XML Data Parse completed in : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d29706",
   "metadata": {},
   "source": [
    "#### Helper Functions to Clean the Data <a id=\"DataClean\"></a>\n",
    "---\n",
    "###### 1. Remove words which does not contribute to context \n",
    "###### 2. Remove Punctuation \n",
    "###### 3. Remove Extra Spaces <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2156ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\\\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\\\n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\\\n",
    "             \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \\\n",
    "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \\\n",
    "             \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \\\n",
    "             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\\\n",
    "             \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\\\n",
    "             \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \\\n",
    "             \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "def Remove_Stopwords(text):\n",
    "    return \" \".join([item for item in text.split(\" \") if item not in stopwords])\n",
    "\n",
    "def Remove_Punctuation(text):\n",
    "    new_text = \"\"\n",
    "    for char in text: \n",
    "        if char in string.punctuation:\n",
    "            new_text = new_text + \" \"\n",
    "        else : \n",
    "            new_text = new_text + char\n",
    "    return new_text  \n",
    "\n",
    "def Remove_Extraspaces(text):\n",
    "    return \" \".join([ item for item in text.split() if item])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9596ee",
   "metadata": {},
   "source": [
    "#### Defination to Apply the Cleaning Methods on Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a060f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apply_Cleaning(data_frame):\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    data_frame[\"Lower\"] = data_frame[\"Text\"].apply(lambda x: x.lower())\n",
    "    data_frame[\"RemovePunctuation\"] = data_frame[\"Lower\"].apply(lambda x: Remove_Punctuation(x))\n",
    "    data_frame[\"RemoveExtraSpaces\"] = data_frame[\"RemovePunctuation\"].apply(lambda x: Remove_Extraspaces(x))\n",
    "    data_frame[\"RemoveStopWords\"] = data_frame[\"RemoveExtraSpaces\"].apply(lambda x: Remove_Stopwords(x))  \n",
    "    print(f'Data Cleaning Completed in {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea9f35",
   "metadata": {},
   "source": [
    "#### Load the GloVe File & Create and Accessible Lookup Dictionary <a id=\"GloVe\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360bb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Glove(glove_file_path):\n",
    "    \n",
    "    '''\n",
    "    The GloVe file contains Embeddings for 400,000 words.\n",
    "    In each line, the first item is a 'word' representation. \n",
    "    And rest of the values in the line, seperated by spaces are it's 50 Embedding values.  \n",
    "    Here we creating dictionary, with each word as a key and it's 50 Embedding representation as value. \n",
    "    '''\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    glove_file = open(glove_file_path, encoding=\"utf8\")\n",
    "    for line in glove_file.readlines():\n",
    "        line_list = line.split(\" \")\n",
    "        temp_list = line_list[1:-1]\n",
    "        temp_list.append(line_list[-1].split(\"\\n\")[0])\n",
    "        float_vector = [vector for vector in np.array(temp_list, dtype=np.float32)]\n",
    "        glove_dict[line_list[0]] = float_vector\n",
    "    glove_file.close()\n",
    "    print(f'Loading GloVe Embedding File completed in : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    print(f'Number of words in vocabulary, having GloVe representation : {len(glove_dict)}')\n",
    "    return glove_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42d1a0",
   "metadata": {},
   "source": [
    "#### Map GloVe Vector for each Word in Sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd61f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glove_Formatting(x):\n",
    "    \n",
    "    '''\n",
    "    Given a sentence, map each word to it's 50 dimenssional embeddings. \n",
    "    If the word is not found in the GloVe dictionary, create 50 Dimenssional vector with all zeros. \n",
    "    '''\n",
    "    \n",
    "    glove_vector = []\n",
    "    for item in x:\n",
    "        try:\n",
    "            glove_vector.append(glove_dict[item])\n",
    "        except:\n",
    "            glove_vector.append([0.0 for i in range(50)])\n",
    "    return np.array(glove_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc8267",
   "metadata": {},
   "source": [
    "#### Map Embeddings of each Word and find the Average of Embeddings for Senetence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1da930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map_GloVe_Embeddings(data_frame):\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    data_frame[\"Embeddings\"] = data_frame[\"RemoveStopWords\"].apply(lambda x : Glove_Formatting(x.split(\" \")) )\n",
    "    data_frame[\"EmbeddingsAverage\"] = data_frame[\"Embeddings\"].apply(lambda x : np.sum(x, axis=0)/len(x))\n",
    "    print(f'Embeddings Mapping for the entire Wiki data, Completed in {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f61d8d",
   "metadata": {},
   "source": [
    "#### Provide Wikipedia XML File Path & Call Parsing Function</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701c6188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia XML file Load completed in  : 10.335050 sec\n",
      "Wikipedia XML Data Parse completed in : 3.941577 sec\n"
     ]
    }
   ],
   "source": [
    "wikipedia_xml = \"enwiki-20210320-pages-articles-multistream1.xml-p1p41242\"\n",
    "dictionary = Parse_Wikipedia_XML(wikipedia_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b50fee",
   "metadata": {},
   "source": [
    "#### Load GloVe File & Create GloVe Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd96a694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embedding File completed in : 7.732581 sec\n",
      "Number of words in vocabulary having GloVe representation : 400000\n"
     ]
    }
   ],
   "source": [
    "glove_file_path = \"glove.6B.50d.txt\"\n",
    "glove_dict = {}\n",
    "glove_dict = Load_Glove(glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24654f0",
   "metadata": {},
   "source": [
    "#### Creating Pandas Data Frame from the XML with Title and Text as Columns & each Row is an entry of each Wiki Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4216b94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anarchism</td>\n",
       "      <td>'''Anarchism''' is a [[political philosophy]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autism</td>\n",
       "      <td>'''Autism''' is a [[developmental disorder]] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albedo</td>\n",
       "      <td>[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>'''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>[[File:Achilles fighting against Memnon Leide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                               Text\n",
       "0  Anarchism   '''Anarchism''' is a [[political philosophy]]...\n",
       "1     Autism   '''Autism''' is a [[developmental disorder]] ...\n",
       "2     Albedo   [[File:Albedo-e hg.svg|thumb|upright=1.3|The ...\n",
       "3    Alabama   '''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...\n",
       "4   Achilles   [[File:Achilles fighting against Memnon Leide..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list = []\n",
    "value_list = []\n",
    "data_frame = pd.DataFrame()\n",
    "for key, value in dictionary.items():\n",
    "    key_list.append(key) \n",
    "    value_list.append(value)\n",
    "data_frame[\"Title\"] = key_list\n",
    "data_frame[\"Text\"]  = value_list\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e1c06",
   "metadata": {},
   "source": [
    "#### Apply Cleaning for Dataframe & derive Embeddings Average for Wiki Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd6a715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Completed in 26.406047 sec\n",
      "Embeddings Mapping for the entire Wiki data, Completed in 47.049976 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lower</th>\n",
       "      <th>RemovePunctuation</th>\n",
       "      <th>RemoveExtraSpaces</th>\n",
       "      <th>RemoveStopWords</th>\n",
       "      <th>Embeddings</th>\n",
       "      <th>EmbeddingsAverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anarchism</td>\n",
       "      <td>'''Anarchism''' is a [[political philosophy]]...</td>\n",
       "      <td>'''anarchism''' is a [[political philosophy]]...</td>\n",
       "      <td>anarchism    is a   political philosophy  ...</td>\n",
       "      <td>anarchism is a political philosophy and politi...</td>\n",
       "      <td>anarchism political philosophy political movem...</td>\n",
       "      <td>[[-0.5725899934768677, -0.22443999350070953, -...</td>\n",
       "      <td>[0.012936445143366903, 0.028217914303868708, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autism</td>\n",
       "      <td>'''Autism''' is a [[developmental disorder]] ...</td>\n",
       "      <td>'''autism''' is a [[developmental disorder]] ...</td>\n",
       "      <td>autism    is a   developmental disorder   ...</td>\n",
       "      <td>autism is a developmental disorder characteriz...</td>\n",
       "      <td>autism developmental disorder characterized di...</td>\n",
       "      <td>[[1.3366999626159668, 0.4839800000190735, -0.0...</td>\n",
       "      <td>[0.06630340237197394, 0.2834236430544985, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albedo</td>\n",
       "      <td>[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...</td>\n",
       "      <td>[[file:albedo-e hg.svg|thumb|upright=1.3|the ...</td>\n",
       "      <td>file albedo e hg svg thumb upright 1 3 the ...</td>\n",
       "      <td>file albedo e hg svg thumb upright 1 3 the per...</td>\n",
       "      <td>file albedo e hg svg thumb upright 1 3 percent...</td>\n",
       "      <td>[[0.2034599930047989, -0.3614400029182434, 1.0...</td>\n",
       "      <td>[0.09905295781556527, 0.47586919074206163, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>'''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...</td>\n",
       "      <td>'''alabama''' ({{ipac-en|,|æ|l|ə|'|b|æ|m|ə|}}...</td>\n",
       "      <td>alabama       ipac en   æ l ə   b æ m ə   ...</td>\n",
       "      <td>alabama ipac en æ l ə b æ m ə is a state in th...</td>\n",
       "      <td>alabama ipac en æ l ə b æ m ə state southeaste...</td>\n",
       "      <td>[[-1.351199984550476, -0.25477999448776245, 0....</td>\n",
       "      <td>[-0.09930973437445643, 0.2312477103479057, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>[[File:Achilles fighting against Memnon Leide...</td>\n",
       "      <td>[[file:achilles fighting against memnon leide...</td>\n",
       "      <td>file achilles fighting against memnon leide...</td>\n",
       "      <td>file achilles fighting against memnon leiden r...</td>\n",
       "      <td>file achilles fighting memnon leiden rijksmuse...</td>\n",
       "      <td>[[0.2034599930047989, -0.3614400029182434, 1.0...</td>\n",
       "      <td>[0.06961334054730832, 0.2537376214703545, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                               Text  \\\n",
       "0  Anarchism   '''Anarchism''' is a [[political philosophy]]...   \n",
       "1     Autism   '''Autism''' is a [[developmental disorder]] ...   \n",
       "2     Albedo   [[File:Albedo-e hg.svg|thumb|upright=1.3|The ...   \n",
       "3    Alabama   '''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...   \n",
       "4   Achilles   [[File:Achilles fighting against Memnon Leide...   \n",
       "\n",
       "                                               Lower  \\\n",
       "0   '''anarchism''' is a [[political philosophy]]...   \n",
       "1   '''autism''' is a [[developmental disorder]] ...   \n",
       "2   [[file:albedo-e hg.svg|thumb|upright=1.3|the ...   \n",
       "3   '''alabama''' ({{ipac-en|,|æ|l|ə|'|b|æ|m|ə|}}...   \n",
       "4   [[file:achilles fighting against memnon leide...   \n",
       "\n",
       "                                   RemovePunctuation  \\\n",
       "0      anarchism    is a   political philosophy  ...   \n",
       "1      autism    is a   developmental disorder   ...   \n",
       "2     file albedo e hg svg thumb upright 1 3 the ...   \n",
       "3      alabama       ipac en   æ l ə   b æ m ə   ...   \n",
       "4     file achilles fighting against memnon leide...   \n",
       "\n",
       "                                   RemoveExtraSpaces  \\\n",
       "0  anarchism is a political philosophy and politi...   \n",
       "1  autism is a developmental disorder characteriz...   \n",
       "2  file albedo e hg svg thumb upright 1 3 the per...   \n",
       "3  alabama ipac en æ l ə b æ m ə is a state in th...   \n",
       "4  file achilles fighting against memnon leiden r...   \n",
       "\n",
       "                                     RemoveStopWords  \\\n",
       "0  anarchism political philosophy political movem...   \n",
       "1  autism developmental disorder characterized di...   \n",
       "2  file albedo e hg svg thumb upright 1 3 percent...   \n",
       "3  alabama ipac en æ l ə b æ m ə state southeaste...   \n",
       "4  file achilles fighting memnon leiden rijksmuse...   \n",
       "\n",
       "                                          Embeddings  \\\n",
       "0  [[-0.5725899934768677, -0.22443999350070953, -...   \n",
       "1  [[1.3366999626159668, 0.4839800000190735, -0.0...   \n",
       "2  [[0.2034599930047989, -0.3614400029182434, 1.0...   \n",
       "3  [[-1.351199984550476, -0.25477999448776245, 0....   \n",
       "4  [[0.2034599930047989, -0.3614400029182434, 1.0...   \n",
       "\n",
       "                                   EmbeddingsAverage  \n",
       "0  [0.012936445143366903, 0.028217914303868708, -...  \n",
       "1  [0.06630340237197394, 0.2834236430544985, 0.12...  \n",
       "2  [0.09905295781556527, 0.47586919074206163, 0.2...  \n",
       "3  [-0.09930973437445643, 0.2312477103479057, 0.0...  \n",
       "4  [0.06961334054730832, 0.2537376214703545, -0.0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = Apply_Cleaning(data_frame)\n",
    "data_frame = Map_GloVe_Embeddings(data_frame)\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc25624",
   "metadata": {},
   "source": [
    "#### Assertion Check for the size of final Average Embedding Dimenssion <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85713100",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_frame)):\n",
    "    assert data_frame[\"EmbeddingsAverage\"][i].shape[0] ==  50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b826c39",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"> Input the Embeddings Vector size, Length of the Population & Datatype to configure Load for FPGA </font> <a id=\"ConfigureDevice\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf69af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorLength = 50\n",
    "NumVectors = len(data_frame)\n",
    "Bytes_Per_value = 4\n",
    "NumDevices = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e9fb5",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"> Configure Population Load in FPGA </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb63b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = xcs.options()\n",
    "opt.vecLength = VectorLength\n",
    "opt.numDevices = NumDevices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d0ee4",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"> U50 having 8GB of HBM Memory. Check if the given Data Load has exceeded the Limit </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06d1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert VectorLength * NumVectors * Bytes_Per_value  <  NumDevices * 8 * 2**30, \"Memory in 1 x U50 is 8GB. Cant Load the Given amount of data into Memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4111f",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"> Load the Population Embeddings into U50 HBM Memeory </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60c24b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = xcs.cosinesim(opt, Bytes_Per_value)\n",
    "cs.startLoadPopulation(NumVectors)\n",
    "for vecNum in range(NumVectors):\n",
    "    vecBuf = cs.getPopulationVectorBuffer(vecNum)\n",
    "\n",
    "    for vecIdx in range(VectorLength):\n",
    "        val = int(data_frame[\"EmbeddingsAverage\"][vecNum][vecIdx]*1000)  # Converting Float32 Value to Int Type\n",
    "        vecBuf.fill(val)\n",
    "    cs.finishCurrentPopulationVector(vecBuf)\n",
    "\n",
    "cs.finishLoadPopulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ee40b",
   "metadata": {},
   "source": [
    "#### <font color=\"red\"> Find the TopK Matchings for the Given Query </font> <a id=\"TopKMatchings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b309f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_TopK_Matchings(query, topK=10):\n",
    "    \n",
    "    '''\n",
    "    Apply the Cleaning, GloVe Mapping function on the given Query.\n",
    "    Call the Xilinx Cosine Simalrity Match Target Vector API to fiind the Top Matchings with the Loaded Population.\n",
    "    Displlay the Mathcing Pages Information.\n",
    "    '''\n",
    "    \n",
    "    query_clean = Remove_Stopwords(Remove_Extraspaces(Remove_Punctuation(query.lower())))\n",
    "    query_embedding = np.sum(Glove_Formatting(query_clean.split(\" \")), axis=0)/len(query_clean.split(\" \"))*1000\n",
    "    targetVec = query_embedding.astype(\"int32\")\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    result = cs.matchTargetVector(topK, targetVec)\n",
    "    print(f'completed in {1000*(time.perf_counter() - tStart):.6f} msec\\n')\n",
    "    print(\"RANK  ID    Wiki Title \\t\\t\\t\\t\\t\\t  MESSAGE \\t\\t\\t\\t        CONFIDENCE\")\n",
    "    print(\"----|-----|-------------|\" + 65 *\"-\" + \"---------------------|---------\")\n",
    "    num = 0\n",
    "    for item in result:\n",
    "        num = num +1\n",
    "        Message = data_frame[\"RemoveStopWords\"][item.index]\n",
    "        print(\"{:02d}\".format(num) + 3*\" \" + \"{:05d}\".format(item.index) + 3*\" \"+ \\\n",
    "              '{message: <10}'.format(message=Remove_Extraspaces(data_frame[\"Title\"][item.index][0:10])) + \\\n",
    "              3*\" \" + Message[0:35] + \" ... \" + Message[-45:-1] + 3* \" \" +\\\n",
    "              '{:.6f}'.format(item.similarity) )\n",
    "    print(f'\\nTopK Matchings completed in {1000*(time.perf_counter() - tStart):.6f} msec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9457e1",
   "metadata": {},
   "source": [
    "---\n",
    "#### <font align=\"center\">Upto this point, it is just a One Time Load & Execution. Now, once all the Data is Loaded, we can run any number of Queries </font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7c13b",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Call the TopK Matchings Function with Your Query Input</font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c922d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 8.611262 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   00020   Agricultur   agricultural science broad multidis ... llusk nematode crop animal production system   0.709433\n",
      "02   16388   DTE          directorate technical education mah ... ation governance body government kerala indi   0.701151\n",
      "03   04379   Economy Fr   economy french guiana tied closely  ...  miquelon economy wallis futuna wallis futun   0.694272\n",
      "04   10938   Rice         file white brown red wild rice jpg  ... h periods different approach fertilizing soi   0.682182\n",
      "05   12403   SDI          list sustainable development goal t ... term un eurostat commonly used sdg indicator   0.681632\n",
      "\n",
      "TopK Matchings completed in 10.051718 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Agriculture\", topK=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16fb730a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 8.967072 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   05859   Harpers Fe   harpers ferry armory second federal ... ing ship united states navy commissioned 199   0.868337\n",
      "02   04009   Escort       escort carrier escort aircraft carr ... ft carriers new construction became availabl   0.866402\n",
      "03   04660   Frigate      frigate ipac en ˈ f r ɪ ɡ ə type wa ... word books london 2005 isbn 1 84415 301 0 re   0.864447\n",
      "04   05837   H M S Drea   several ships one submarine royal n ... rine periscope publishing isbn 978 190438109   0.862728\n",
      "05   13333   Torpedo      file ataquechocrane png thumb torpe ... ip blockading fleet form asymmetrical warfar   0.859361\n",
      "06   02805   Cruiser      file uss port royal cg 73 jpg thumb ... eavy cruiser design designated cruiser kille   0.857451\n",
      "07   07278   Kriegsmari   kriegsmarine ipa de ˈkʁiːksmaˌʁiːnə ... ffectiveness surface commerce raiders convoy   0.856885\n",
      "08   01766   Battle Jut   please change following result with ... talling 250 ships them—directly engaged twic   0.856547\n",
      "09   04268   Etna         etna class cruiser etna class cruis ... ack cargo ship originally built united state   0.856206\n",
      "10   03520   Destroyer    file uss winston churchill jpg thum ... cement 9600 tons thus growing size almost 34   0.855035\n",
      "\n",
      "TopK Matchings completed in 10.097426 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Battle Ships\", topK=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a46e53d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 8.704400 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   13998   World War    german junkers ju 87 stuka dive bom ... tres development nuclear weapons two uses wa   0.935970\n",
      "02   04364   French Arm   french armed forces lang fr forces  ...  war independence italy prussia within franc   0.924047\n",
      "03   13312   Twilight     twilight 2000 1984 post apocalyptic ... ow intensity conflict low intensity civil wa   0.922727\n",
      "04   03825   Erwin Romm   johannes erwin eugen rommel 15 nove ... sert fox storied military career erwin romme   0.916473\n",
      "05   14010   Battle Mon   battle monte cassino also known bat ... would fall october 1943 proved far optimisti   0.912933\n",
      "06   06063   Foreign It   foreign relations italian republic  ... aly eight decade experience colonialism ende   0.909587\n",
      "07   05433   History Ge   concept germany distinct region cen ... do kleinhubbert access date april 23 2020 re   0.907916\n",
      "08   10174   Peloponnes   peloponnesian war 431–404 bc ref na ... umb fragment athenian tribute list 425–424 b   0.907843\n",
      "09   03613   Dual Allia   file dual alliance 1914 png 300px t ... ut war known central powers ended defeat 191   0.906554\n",
      "10   09186   Nagasaki     see template infobox settlement add ... ving intermediaries two east asia n neighbor   0.906547\n",
      "11   13314   Treaty Bre   treaty brest litovsk also known pea ... ast central europe 1917 1918 2019 pp 3 38 re   0.905964\n",
      "12   13470   Foreign Un   diplomatic foreign relations united ... t list largest empires largest empire histor   0.905912\n",
      "13   01441   Bernard Mo   field marshal united kingdom field  ... eath east hamburg surrender berlin ussr 2 ma   0.904603\n",
      "14   13462   History Un   file articles union jpg thumb publi ... land scotland also continued system educatio   0.904585\n",
      "15   05467   History Re   turkey republic turkey created abol ... atified grand national assembly april 20 192   0.904056\n",
      "16   01613   Battle Sta   file eastern front 1942 05 1942 11  ... don 2011 p 96 ref ref name georgyzhukov rp 8   0.903766\n",
      "17   01016   Australian   australian army military land force ... d publishers year 2018 isbn 9781760790479 re   0.902754\n",
      "18   13374   Champion U   champion universe tryco slatterus c ... would kept buried ref hulk 2004 series 11 re   0.902471\n",
      "19   09478   Operation    according german army wehrmacht ger ... ly failed resulted wehrmacht retreat collaps   0.902443\n",
      "20   11419   History So   history south korea formally begins ... ll us soviet joint commission ko 미소공동위원회 194   0.902237\n",
      "\n",
      "TopK Matchings completed in 10.520362 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Second World War\", topK=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a329f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 8.990805 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   04120   Experiment   experimental cancer treatments non  ... gns optimism bias irrational belief beat odd   0.819006\n",
      "02   03325   Duesberg     duesberg hypothesis claim associate ...  hypothesis basis fact ref name drugusenatur   0.793924\n",
      "03   06130   I R S        leibniz institute research society  ... tional football competition australia irelan   0.786492\n",
      "04   02849   Chemothera   chemotherapy often abbreviated chem ... rolong life palliative care palliate symptom   0.783844\n",
      "05   15548   Psilocybin   psilocybin efn synonyms alternate s ... c drugs artists creativity ref name berge199   0.780460\n",
      "06   01742   Geneticall   noinclude good article noinclude te ...  gm food labeled status gene edited organism   0.779421\n",
      "07   02559   Carcinogen   carcinogen substance radionuclide r ... onvert less toxic carcinogen toxic carcinoge   0.776690\n",
      "08   16220   Darbepoeti   darbepoetin alfa international nonp ... 3c9 8530865db3f9 access date 7 april 2020 re   0.776593\n",
      "09   15007   Infection    infection invasion organism body ti ... gov language en us access date 2019 12 09 re   0.773413\n",
      "10   05733   Hershey Ch   hershey–chase experiments series ex ... lfur labeled phages phosphorus labeled phage   0.772951\n",
      "11   06397   Inhalant     inhalants broad range household ind ...  drugs pharmaceutical products used illicitl   0.769409\n",
      "12   10535   Phenothiaz   phenothiazine abbreviated ptz organ ... 2580516 doi 10 1016 s1734 1140 12 70726 0 re   0.769163\n",
      "13   06019   Infusion     infusion refers process extracting  ... uid substances directly vein medical purpose   0.768288\n",
      "14   15386   Narcotic     term narcotic ipac en n ɑːr ˈ k ɒ ᵻ ... control 116 drugs classifies narcotic includ   0.767461\n",
      "15   10310   Pharmacolo   pharmacology branch medicine pharma ... ugs purified substance adulterated substance   0.766056\n",
      "\n",
      "TopK Matchings completed in 11.178289 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Drug Discovery\", topK=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f01145d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 8.971302 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   08009   Foreign Ma   malawi former president bakili mulu ... atute international criminal court article 9   0.917747\n",
      "02   07413   Foreign La   foreign relations latvia primary re ...  sweden switzerland thailand turkey venezuel   0.915774\n",
      "03   12759   Foreign Tr   modern trinidad tobago maintains cl ...  represented governor general trinidad tobag   0.913249\n",
      "04   06458   Foreign Ja   jamaica diplomatic relations nation ... coordinating discussions invigorating societ   0.912985\n",
      "05   13583   United Nat   united nations trusteeship council  ... rust territory united states permanent membe   0.909936\n",
      "06   02116   Foreign Ca   cameroon noncontentious low profile ... es well memberships francophonie commonwealt   0.907410\n",
      "07   07465   Foreign Li   lithuania country south eastern sho ... tion security cooperation europe also succes   0.906546\n",
      "08   13580   United Nat   list members united nations securit ... calated world war ii sfn kennedy 2006 p 13–2   0.903739\n",
      "09   11661   Foreign So   foreign relations south africa span ... azi people swazis long ago part swazi kingdo   0.900686\n",
      "10   10801   Foreign Re   two decades preceding republic cong ... dex aspx lang eng embassy canada kinshasa re   0.898769\n",
      "\n",
      "TopK Matchings completed in 10.151449 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"United Nations\", topK=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa5f5e",
   "metadata": {},
   "source": [
    "### <center> End of the Notebook </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
