{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec5ee12",
   "metadata": {},
   "source": [
    "# Welcome to Xilinx Cosine Similarity Acceleration Demo \n",
    "---\n",
    "\n",
    "**This Notebook demonstrates how to use the Xilinx Cosine Similarity product and shows the power of Xilinx FPGAs to accelerate Cosine Similarity**\n",
    "\n",
    "---\n",
    "\n",
    "### The Demo : Wiki Search Engine \n",
    "\n",
    "In this Demo Example, we will create Search Engine based on Wikipedia Data. \n",
    "\n",
    "The User will provide <u>Keyword</u> ( or ) <u>Context Phrase</u> to search for the related information.\n",
    "\n",
    "This Example will take the given Keyword / Phrase and filter out the Wikipedia Documents and returns the Top Matching Information. \n",
    "\n",
    "The Top Matching are calculated based on similarity between the given Keyword and all Wikipedia Pages. This similarity is know as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "\n",
    "Instead of finding Similarity with the direct one-hot word representation, we used [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) Word Embeddings, which maps words into more meaningful space.\n",
    "\n",
    "In General, finding Cosine Similarity on large dataset will take a huge amount of time on CPU \n",
    "\n",
    "With the Xilinx Cosine Similarity Acceleration, it will speedup the process by > ~ 80x\n",
    "\n",
    "We will use the Xilinx Cosine Similarity module (**xilCosineSim**) and setup a population against which similarity of target vectors can be calculated.\n",
    "\n",
    " \n",
    "### The Demo is Structured in Six Sections :\n",
    "1. [**Download Wikipedia Data & GloVe Embeddings File**](#DownloadFiles)\n",
    "<br><br>\n",
    "2. [**Load and Parse Wikipedia XML File**](#LoadandParse)\n",
    "<br><br>\n",
    "3. [**Clean the XML Data**](#DataClean)\n",
    "<br><br>\n",
    "4. [**Calculate the Embeddings Representation for All Wiki Pages**](#GloVe)\n",
    "<br><br>\n",
    "5. [**Load the Embeddings representation of Wiki Pages into U50 HBM Memory**](#ConfigureDevice)\n",
    "<br><br>\n",
    "6. [**Run Cosine Similarity to Find out the TopK Matchings for the given Query**](#TopKMatchings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1475de4",
   "metadata": {},
   "source": [
    " #### Load Xilinx Cosine Similarity Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ebeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xilCosineSim as xcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca1383",
   "metadata": {},
   "source": [
    "#### Load Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd77e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76fbce",
   "metadata": {},
   "source": [
    "#### Download the Wikipedia Data Dump <a id=\"DownloadFiles\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0336cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Wikipedia File ...\n",
      "Download Completed !!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\") :\n",
    "    print(\"Downloading Wikipedia File ...\")\n",
    "    os.system(\"wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\")\n",
    "    print(\"Download Completed !!\")\n",
    "if not os.path.isfile(\"enwiki-latest-pages-articles-multistream1.xml-p1p41242\") :\n",
    "    os.system(\"bzip2 -d enwiki-latest-pages-articles-multistream1.xml-p1p41242.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd36393",
   "metadata": {},
   "source": [
    "#### Download the GloVe File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68161958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe Embedding File ...\n",
      "Download Completed !!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"glove.6B.50d.txt.tar\") :\n",
    "    print(\"Downloading GloVe Embedding File ...\")\n",
    "    os.system(\"wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ogyMmAu0fcZBdSwTQJuX6jHLzlTJnql0' -O glove.6B.50d.txt.tar\")\n",
    "    print(\"Download Completed !!\")\n",
    "if not os.path.isfile(\"glove.6B.50d.txt\") :\n",
    "    os.system(\"tar -xvzf glove.6B.50d.txt.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd001d2f",
   "metadata": {},
   "source": [
    "#### Parsing Load and Parse the Wikipedia XML File <a id=\"LoadandParse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b64b238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse_Wikipedia_XML(wikipedia_xml, Max_Pages=None):\n",
    "    \n",
    "    '''\n",
    "    The Input is a wikipedia xml compressed (whole/partial) file. \n",
    "    This file can be downloaded from https://dumps.wikimedia.org/enwiki/ \n",
    "    To decompress, Use : bzip2 -d <bz2_compressed_xml_file_path>\n",
    "    \n",
    "    Parse the XML file from root. This defination parse the children of roots and go through its modules. \n",
    "    Every child with a 'page' as attribute is a single wikipedia page. \n",
    "    The XML file is consisted with multiple such pages. \n",
    "    In each page, it contains title, text and other metadata items. \n",
    "    We make a dictionary with title as key & text as value.\n",
    "    '''\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    tree = ET.parse(wikipedia_xml)\n",
    "    print(f'Wikipedia XML file Load completed in  : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    tStart = time.perf_counter()\n",
    "    root = tree.getroot()\n",
    "    dictionary = {}\n",
    "    if Max_Pages != None : \n",
    "        root = root[0:Max_Pages]\n",
    "    for child in root :\n",
    "        if \"page\" in child.tag : \n",
    "            for branch in child:\n",
    "                if \"title\" in branch.tag:\n",
    "                    if branch.text.isupper() :\n",
    "                        title = branch.text\n",
    "                    else : \n",
    "                        title_list = re.findall('[A-Z][a-z]*', branch.text)\n",
    "                        title = \" \".join(title_list)\n",
    "                    dictionary[title] = \"\"\n",
    "                if \"redirect\" in branch.tag :\n",
    "                    dictionary.pop(title)\n",
    "                if \"revision\" in branch.tag : \n",
    "                    for chunk in branch:\n",
    "                        if \"text\" in chunk.tag:\n",
    "                            number = 0\n",
    "                            for line in chunk.text.split(\"\\n\") : \n",
    "                                if \"|\" not in line[0:5] and \"{{\" not in line[0:5] and \"}}\" not in line[0:5]:\n",
    "                                    if len(line) > 100 : \n",
    "                                        number = number + 1\n",
    "                                        try:\n",
    "                                            dictionary[title] = dictionary[title] + \" \" + line\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                        if number > 5: # Only Read First Five Paragraphs \n",
    "                                            break\n",
    "    print(f'Wikipedia XML Data Parse completed in : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639690a4",
   "metadata": {},
   "source": [
    "#### Helper Functions to Clean the Data <a id=\"DataClean\"></a>\n",
    "---\n",
    "###### 1. Remove words which does not contribute to context \n",
    "###### 2. Remove Punctuation \n",
    "###### 3. Remove Extra Spaces <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9efb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\\\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\\\n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\\\n",
    "             \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \\\n",
    "              \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \\\n",
    "             \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \\\n",
    "             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\\\n",
    "             \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\\\n",
    "             \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \\\n",
    "             \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "def Remove_Stopwords(text):\n",
    "    return \" \".join([item for item in text.split(\" \") if item not in stopwords])\n",
    "\n",
    "def Remove_Punctuation(text):\n",
    "    new_text = \"\"\n",
    "    for char in text: \n",
    "        if char in string.punctuation:\n",
    "            new_text = new_text + \" \"\n",
    "        else : \n",
    "            new_text = new_text + char\n",
    "    return new_text  \n",
    "\n",
    "def Remove_Extraspaces(text):\n",
    "    return \" \".join([ item for item in text.split() if item])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874e3b8",
   "metadata": {},
   "source": [
    "#### Defination to Apply the Cleaning Methods on Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97713c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apply_Cleaning(data_frame):\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    data_frame[\"Lower\"] = data_frame[\"Text\"].apply(lambda x: x.lower())\n",
    "    data_frame[\"RemovePunctuation\"] = data_frame[\"Lower\"].apply(lambda x: Remove_Punctuation(x))\n",
    "    data_frame[\"RemoveExtraSpaces\"] = data_frame[\"RemovePunctuation\"].apply(lambda x: Remove_Extraspaces(x))\n",
    "    data_frame[\"RemoveStopWords\"] = data_frame[\"RemoveExtraSpaces\"].apply(lambda x: Remove_Stopwords(x))  \n",
    "    print(f'Data Cleaning Completed in {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa76643",
   "metadata": {},
   "source": [
    "#### Load the GloVe File & Create and Accessible Lookup Dictionary <a id=\"GloVe\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de788eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Glove(glove_file_path):\n",
    "    \n",
    "    '''\n",
    "    The GloVe file contains Embeddings for 400,000 words.\n",
    "    In each line, the first item is a 'word' representation. \n",
    "    And rest of the values in the line, seperated by spaces are it's 50 Embedding values.  \n",
    "    Here we creating dictionary, with each word as a key and it's 50 Embedding representation as value. \n",
    "    '''\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    glove_file = open(glove_file_path, encoding=\"utf8\")\n",
    "    for line in glove_file.readlines():\n",
    "        line_list = line.split(\" \")\n",
    "        temp_list = line_list[1:-1]\n",
    "        temp_list.append(line_list[-1].split(\"\\n\")[0])\n",
    "        float_vector = [vector for vector in np.array(temp_list, dtype=np.float32)]\n",
    "        glove_dict[line_list[0]] = float_vector\n",
    "    glove_file.close()\n",
    "    print(f'Loading GloVe Embedding File completed in : {(time.perf_counter() - tStart):.6f} sec')\n",
    "    print(f'Number of words in vocabulary, having GloVe representation : {len(glove_dict)}')\n",
    "    return glove_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d570a",
   "metadata": {},
   "source": [
    "#### Map GloVe Vector for each Word in Sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c901f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glove_Formatting(x):\n",
    "    \n",
    "    '''\n",
    "    Given a sentence, map each word to it's 50 dimenssional embeddings. \n",
    "    If the word is not found in the GloVe dictionary, create 50 Dimenssional vector with all zeros. \n",
    "    '''\n",
    "    \n",
    "    glove_vector = []\n",
    "    for item in x:\n",
    "        try:\n",
    "            glove_vector.append(glove_dict[item])\n",
    "        except:\n",
    "            glove_vector.append([0.0 for i in range(50)])\n",
    "    return np.array(glove_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c312b74",
   "metadata": {},
   "source": [
    "#### Map Embeddings of each Word and find the Average of Embeddings for Senetence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b367ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Map_GloVe_Embeddings(data_frame):\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    data_frame[\"Embeddings\"] = data_frame[\"RemoveStopWords\"].apply(lambda x : Glove_Formatting(x.split(\" \")) )\n",
    "    data_frame[\"EmbeddingsAverage\"] = data_frame[\"Embeddings\"].apply(lambda x : np.sum(x, axis=0)/len(x))\n",
    "    print(f'Embeddings Mapping for the entire Wiki data, Completed in {(time.perf_counter() - tStart):.6f} sec')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46612621",
   "metadata": {},
   "source": [
    "#### Provide Wikipedia XML File Path & Call Parsing Function</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "870cc751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia XML file Load completed in  : 8.053361 sec\n",
      "Wikipedia XML Data Parse completed in : 2.622177 sec\n"
     ]
    }
   ],
   "source": [
    "wikipedia_xml = \"enwiki-latest-pages-articles-multistream1.xml-p1p41242\"\n",
    "dictionary = Parse_Wikipedia_XML(wikipedia_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0006a06",
   "metadata": {},
   "source": [
    "#### Load GloVe File & Create GloVe Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538fffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embedding File completed in : 6.805943 sec\n",
      "Number of words in vocabulary, having GloVe representation : 400000\n"
     ]
    }
   ],
   "source": [
    "glove_file_path = \"glove.6B.50d.txt\"\n",
    "glove_dict = {}\n",
    "glove_dict = Load_Glove(glove_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117d579",
   "metadata": {},
   "source": [
    "#### Creating Pandas Data Frame from the XML with Title and Text as Columns & each Row is an entry of each Wiki Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5edfb1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anarchism</td>\n",
       "      <td>'''Anarchism''' is a [[political philosophy]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autism</td>\n",
       "      <td>'''Autism''' is a [[developmental disorder]] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albedo</td>\n",
       "      <td>[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>'''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>[[File:Achilles fighting against Memnon Leide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                               Text\n",
       "0  Anarchism   '''Anarchism''' is a [[political philosophy]]...\n",
       "1     Autism   '''Autism''' is a [[developmental disorder]] ...\n",
       "2     Albedo   [[File:Albedo-e hg.svg|thumb|upright=1.3|The ...\n",
       "3    Alabama   '''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...\n",
       "4   Achilles   [[File:Achilles fighting against Memnon Leide..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list = []\n",
    "value_list = []\n",
    "data_frame = pd.DataFrame()\n",
    "for key, value in dictionary.items():\n",
    "    key_list.append(key) \n",
    "    value_list.append(value)\n",
    "data_frame[\"Title\"] = key_list\n",
    "data_frame[\"Text\"]  = value_list\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1733f",
   "metadata": {},
   "source": [
    "#### Apply Cleaning for Dataframe & derive Embeddings Average for Wiki Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed7ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Completed in 17.073280 sec\n",
      "Embeddings Mapping for the entire Wiki data, Completed in 39.806676 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lower</th>\n",
       "      <th>RemovePunctuation</th>\n",
       "      <th>RemoveExtraSpaces</th>\n",
       "      <th>RemoveStopWords</th>\n",
       "      <th>Embeddings</th>\n",
       "      <th>EmbeddingsAverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anarchism</td>\n",
       "      <td>'''Anarchism''' is a [[political philosophy]]...</td>\n",
       "      <td>'''anarchism''' is a [[political philosophy]]...</td>\n",
       "      <td>anarchism    is a   political philosophy  ...</td>\n",
       "      <td>anarchism is a political philosophy and politi...</td>\n",
       "      <td>anarchism political philosophy political movem...</td>\n",
       "      <td>[[-0.5725899934768677, -0.22443999350070953, -...</td>\n",
       "      <td>[0.012981447413275738, 0.024957372031251243, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autism</td>\n",
       "      <td>'''Autism''' is a [[developmental disorder]] ...</td>\n",
       "      <td>'''autism''' is a [[developmental disorder]] ...</td>\n",
       "      <td>autism    is a   developmental disorder   ...</td>\n",
       "      <td>autism is a developmental disorder characteriz...</td>\n",
       "      <td>autism developmental disorder characterized di...</td>\n",
       "      <td>[[1.3366999626159668, 0.4839800000190735, -0.0...</td>\n",
       "      <td>[0.09803162424081913, 0.2675378602812644, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albedo</td>\n",
       "      <td>[[File:Albedo-e hg.svg|thumb|upright=1.3|The ...</td>\n",
       "      <td>[[file:albedo-e hg.svg|thumb|upright=1.3|the ...</td>\n",
       "      <td>file albedo e hg svg thumb upright 1 3 the ...</td>\n",
       "      <td>file albedo e hg svg thumb upright 1 3 the per...</td>\n",
       "      <td>file albedo e hg svg thumb upright 1 3 percent...</td>\n",
       "      <td>[[0.2034599930047989, -0.3614400029182434, 1.0...</td>\n",
       "      <td>[0.10138701840600607, 0.4683248054349573, 0.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>'''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...</td>\n",
       "      <td>'''alabama''' ({{ipac-en|,|æ|l|ə|'|b|æ|m|ə|}}...</td>\n",
       "      <td>alabama       ipac en   æ l ə   b æ m ə   ...</td>\n",
       "      <td>alabama ipac en æ l ə b æ m ə is a state in th...</td>\n",
       "      <td>alabama ipac en æ l ə b æ m ə state southeaste...</td>\n",
       "      <td>[[-1.351199984550476, -0.25477999448776245, 0....</td>\n",
       "      <td>[-0.10759588041018844, 0.23540118394727452, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>[[File:Achilles fighting against Memnon Leide...</td>\n",
       "      <td>[[file:achilles fighting against memnon leide...</td>\n",
       "      <td>file achilles fighting against memnon leide...</td>\n",
       "      <td>file achilles fighting against memnon leiden r...</td>\n",
       "      <td>file achilles fighting memnon leiden rijksmuse...</td>\n",
       "      <td>[[0.2034599930047989, -0.3614400029182434, 1.0...</td>\n",
       "      <td>[0.06961334054730832, 0.2537376214703545, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                               Text  \\\n",
       "0  Anarchism   '''Anarchism''' is a [[political philosophy]]...   \n",
       "1     Autism   '''Autism''' is a [[developmental disorder]] ...   \n",
       "2     Albedo   [[File:Albedo-e hg.svg|thumb|upright=1.3|The ...   \n",
       "3    Alabama   '''Alabama''' ({{IPAc-en|,|æ|l|ə|'|b|æ|m|ə|}}...   \n",
       "4   Achilles   [[File:Achilles fighting against Memnon Leide...   \n",
       "\n",
       "                                               Lower  \\\n",
       "0   '''anarchism''' is a [[political philosophy]]...   \n",
       "1   '''autism''' is a [[developmental disorder]] ...   \n",
       "2   [[file:albedo-e hg.svg|thumb|upright=1.3|the ...   \n",
       "3   '''alabama''' ({{ipac-en|,|æ|l|ə|'|b|æ|m|ə|}}...   \n",
       "4   [[file:achilles fighting against memnon leide...   \n",
       "\n",
       "                                   RemovePunctuation  \\\n",
       "0      anarchism    is a   political philosophy  ...   \n",
       "1      autism    is a   developmental disorder   ...   \n",
       "2     file albedo e hg svg thumb upright 1 3 the ...   \n",
       "3      alabama       ipac en   æ l ə   b æ m ə   ...   \n",
       "4     file achilles fighting against memnon leide...   \n",
       "\n",
       "                                   RemoveExtraSpaces  \\\n",
       "0  anarchism is a political philosophy and politi...   \n",
       "1  autism is a developmental disorder characteriz...   \n",
       "2  file albedo e hg svg thumb upright 1 3 the per...   \n",
       "3  alabama ipac en æ l ə b æ m ə is a state in th...   \n",
       "4  file achilles fighting against memnon leiden r...   \n",
       "\n",
       "                                     RemoveStopWords  \\\n",
       "0  anarchism political philosophy political movem...   \n",
       "1  autism developmental disorder characterized di...   \n",
       "2  file albedo e hg svg thumb upright 1 3 percent...   \n",
       "3  alabama ipac en æ l ə b æ m ə state southeaste...   \n",
       "4  file achilles fighting memnon leiden rijksmuse...   \n",
       "\n",
       "                                          Embeddings  \\\n",
       "0  [[-0.5725899934768677, -0.22443999350070953, -...   \n",
       "1  [[1.3366999626159668, 0.4839800000190735, -0.0...   \n",
       "2  [[0.2034599930047989, -0.3614400029182434, 1.0...   \n",
       "3  [[-1.351199984550476, -0.25477999448776245, 0....   \n",
       "4  [[0.2034599930047989, -0.3614400029182434, 1.0...   \n",
       "\n",
       "                                   EmbeddingsAverage  \n",
       "0  [0.012981447413275738, 0.024957372031251243, -...  \n",
       "1  [0.09803162424081913, 0.2675378602812644, 0.12...  \n",
       "2  [0.10138701840600607, 0.4683248054349573, 0.24...  \n",
       "3  [-0.10759588041018844, 0.23540118394727452, 0....  \n",
       "4  [0.06961334054730832, 0.2537376214703545, -0.0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = Apply_Cleaning(data_frame)\n",
    "data_frame = Map_GloVe_Embeddings(data_frame)\n",
    "\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2430b",
   "metadata": {},
   "source": [
    "#### Assertion Check for the size of final Average Embedding Dimenssion <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0c3b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_frame)):\n",
    "    assert data_frame[\"EmbeddingsAverage\"][i].shape[0] ==  50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd5c83",
   "metadata": {},
   "source": [
    "#### Input the Embeddings Vector size, Length of the Population & Datatype to configure Load for FPGA  <a id=\"ConfigureDevice\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0355a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorLength = 50\n",
    "NumVectors = len(data_frame)\n",
    "Bytes_Per_value = 4\n",
    "NumDevices = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3188f",
   "metadata": {},
   "source": [
    "####  Configure Population Load in FPGA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814fcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = xcs.options()\n",
    "opt.vecLength = VectorLength\n",
    "opt.numDevices = NumDevices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbcc4a",
   "metadata": {},
   "source": [
    "#### U50 having 8GB of HBM Memory. Check if the given Data Load has exceeded the Limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47967ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert VectorLength * NumVectors * Bytes_Per_value  <  NumDevices * 8 * 2**30, \"Memory in 1 x U50 is 8GB. Cant Load the Given amount of data into Memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7b72b",
   "metadata": {},
   "source": [
    "#### Load the Population Embeddings into U50 HBM Memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba44dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = xcs.cosinesim(opt, Bytes_Per_value)\n",
    "cs.startLoadPopulation(NumVectors)\n",
    "for vecNum in range(NumVectors):\n",
    "    vecBuf = cs.getPopulationVectorBuffer(vecNum)\n",
    "\n",
    "    valVec = []\n",
    "    for vecIdx in range(VectorLength):\n",
    "        valVec.append((int(data_frame[\"EmbeddingsAverage\"][vecNum][vecIdx]*1000)))  # Converting Float32 Value to Int Type\n",
    "    vecBuf.append(valVec)\n",
    "    cs.finishCurrentPopulationVector(vecBuf)\n",
    "\n",
    "cs.finishLoadPopulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18e629",
   "metadata": {},
   "source": [
    "####  Find the TopK Matchings for the Given Query <a id=\"TopKMatchings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af4be072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_TopK_Matchings(query, topK=10):\n",
    "    \n",
    "    '''\n",
    "    Apply the Cleaning, GloVe Mapping function on the given Query.\n",
    "    Call the Xilinx Cosine Simalrity Match Target Vector API to fiind the Top Matchings with the Loaded Population.\n",
    "    Displlay the Mathcing Pages Information.\n",
    "    '''\n",
    "    \n",
    "    query_clean = Remove_Stopwords(Remove_Extraspaces(Remove_Punctuation(query.lower())))\n",
    "    query_embedding = np.sum(Glove_Formatting(query_clean.split(\" \")), axis=0)/len(query_clean.split(\" \"))*1000\n",
    "    targetVec = query_embedding.astype(\"int32\")\n",
    "    \n",
    "    tStart = time.perf_counter()\n",
    "    result = cs.matchTargetVector(topK, targetVec)\n",
    "    print(f'completed in {1000*(time.perf_counter() - tStart):.6f} msec\\n')\n",
    "    print(\"RANK  ID    Wiki Title \\t\\t\\t\\t\\t\\t  MESSAGE \\t\\t\\t\\t        CONFIDENCE\")\n",
    "    print(\"----|-----|-------------|\" + 65 *\"-\" + \"---------------------|---------\")\n",
    "    num = 0\n",
    "    for item in result:\n",
    "        num = num +1\n",
    "        Message = data_frame[\"RemoveStopWords\"][item.index]\n",
    "        print(\"{:02d}\".format(num) + 3*\" \" + \"{:05d}\".format(item.index) + 3*\" \"+ \\\n",
    "              '{message: <10}'.format(message=Remove_Extraspaces(data_frame[\"Title\"][item.index][0:10])) + \\\n",
    "              3*\" \" + Message[0:35] + \" ... \" + Message[-45:-1] + 3* \" \" +\\\n",
    "              '{:.6f}'.format(item.similarity) )\n",
    "    print(f'\\nTopK Matchings completed in {1000*(time.perf_counter() - tStart):.6f} msec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f21306",
   "metadata": {},
   "source": [
    "---\n",
    "#### Upto this point, it is just a One Time Load & Execution. Now, once all the Data is Loaded, we can run any number of Queries \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577706e6",
   "metadata": {},
   "source": [
    "#### Call the TopK Matchings Function with Your Query Input\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d758a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 1.917142 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   00465   Aquacultur   file world capture fisheries aquacu ... production food fish aquatic plants 1990–201   0.709072\n",
      "02   16389   DTE          directorate technical education mah ... ation governance body government kerala indi   0.701151\n",
      "03   04379   Economy Fr   economy french guiana tied closely  ...  miquelon economy wallis futuna wallis futun   0.694272\n",
      "04   15095   Fertilizer   file lite trac spreader jpg thumb l ... bsite world data access date 7 march 2020 re   0.679945\n",
      "05   12514   SADC         southern african development commun ... can development coordination conference sadc   0.672894\n",
      "\n",
      "TopK Matchings completed in 3.379484 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Agriculture\", topK=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3fbd807",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 3.854516 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   04009   Escort       escort carrier escort aircraft carr ... ft carriers new construction became availabl   0.869463\n",
      "02   05859   Harpers Fe   harpers ferry armory second federal ... ing ship united states navy commissioned 199   0.868337\n",
      "03   05837   H M S Drea   several ships one submarine royal n ... rine periscope publishing isbn 978 190438109   0.862728\n",
      "04   13335   Torpedo      file ataquechocrane png thumb torpe ... ip blockading fleet form asymmetrical warfar   0.859361\n",
      "05   02804   Cruiser      file uss port royal cg 73 jpg thumb ... eavy cruiser design designated cruiser kille   0.858787\n",
      "06   04660   Frigate      frigate ipac en ˈ f r ɪ ɡ ə type wa ... word books london 2005 isbn 1 84415 301 0 re   0.858240\n",
      "07   07278   Kriegsmari   kriegsmarine ipa de ˈkʁiːksmaˌʁiːnə ... ffectiveness surface commerce raiders convoy   0.856885\n",
      "08   01766   Battle Jut   please change following result with ... talling 250 ships them—directly engaged twic   0.856547\n",
      "09   04268   Etna         etna class cruiser etna class cruis ... ack cargo ship originally built united state   0.856206\n",
      "10   01502   Battlecrui   file hms hood 51 march 17 1924 jpg  ... rmoured deck protecting engines simply armou   0.853573\n",
      "\n",
      "TopK Matchings completed in 4.840378 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Battle Ships\", topK=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "861533d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 2.881894 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   14000   World War    german junkers ju 87 stuka dive bom ... war crimes war crimes trials japanese leader   0.936312\n",
      "02   04364   French Arm   french armed forces lang fr forces  ...  war independence italy prussia within franc   0.923572\n",
      "03   13314   Twilight     twilight 2000 1984 post apocalyptic ... ow intensity conflict low intensity civil wa   0.922851\n",
      "04   01880   Balkan War   balkan wars consisted two conflicts ... man army assassinated young turks due failur   0.916367\n",
      "05   14012   Battle Mon   battle monte cassino also known bat ... would fall october 1943 proved far optimisti   0.912933\n",
      "06   03825   Erwin Romm   johannes erwin eugen rommel 15 nove ... 1 polish italian descent sfn butler 2015 p 3   0.909934\n",
      "07   06063   Foreign It   foreign relations italian republic  ... aly eight decade experience colonialism ende   0.909511\n",
      "08   13316   Treaty Bre   treaty brest litovsk also known pea ... ast central europe 1917 1918 2019 pp 3 38 re   0.908696\n",
      "09   10176   Peloponnes   peloponnesian war 431–404 bc ref na ... umb fragment athenian tribute list 425–424 b   0.907843\n",
      "10   03613   Dual Allia   file dual alliance 1914 png 300px t ... ut war known central powers ended defeat 191   0.906554\n",
      "11   05433   History Ge   concept germany distinct region cen ... do kleinhubbert access date april 23 2020 re   0.906355\n",
      "12   13472   Foreign Un   diplomatic foreign relations united ... t list largest empires largest empire histor   0.905912\n",
      "13   09188   Nagasaki     see template infobox settlement add ... ving intermediaries two east asia n neighbor   0.905881\n",
      "14   02650   Central Po   file leaders central powers vierbun ... ta trade paperback isbn 978 0 553 38240 2 re   0.905628\n",
      "15   01441   Bernard Mo   field marshal united kingdom field  ... ath south hamburg surrender berlin ussr 2 ma   0.904827\n",
      "16   05467   History Re   turkey republic turkey created abol ... atified grand national assembly april 20 192   0.904452\n",
      "17   13464   History Un   file articles union jpg thumb publi ... land scotland also continued system educatio   0.903444\n",
      "18   05872   Second Pol   second polish republic commonly kno ... t bitter glory poland fate 1918–1939 1998 re   0.903316\n",
      "19   06440   Joseph Sta   georgy malenkov malenkov efn forced ... efiore 3y 2007 3p 23 4a1 kotkin 4y 2014 4p 1   0.902829\n",
      "20   13376   Champion U   champion universe tryco slatterus c ... would kept buried ref hulk 2004 series 11 re   0.902471\n",
      "\n",
      "TopK Matchings completed in 4.371948 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Second World War\", topK=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8172593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 2.283171 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   04120   Experiment   experimental cancer treatments non  ... gns optimism bias irrational belief beat odd   0.819006\n",
      "02   03324   Duesberg     duesberg hypothesis claim associate ...  hypothesis basis fact ref name drugusenatur   0.793924\n",
      "03   06130   I R S        leibniz institute research society  ... tional football competition australia irelan   0.786492\n",
      "04   02848   Chemothera   chemotherapy often abbreviated chem ... rolong life palliative care palliate symptom   0.784181\n",
      "05   01742   Geneticall   noinclude good article noinclude te ...  gm food labeled status gene edited organism   0.779460\n",
      "06   02558   Carcinogen   carcinogen substance radionuclide r ... onvert less toxic carcinogen toxic carcinoge   0.776690\n",
      "07   16222   Darbepoeti   darbepoetin alfa international nonp ... 3c9 8530865db3f9 access date 7 april 2020 re   0.776593\n",
      "08   15550   Psilocybin   psilocybin efn synonyms alternate s ... c drugs artists creativity ref name berge199   0.774467\n",
      "09   15388   Narcotic     term narcotic ipac en n ɑːr ˈ k ɒ ᵻ ... control 116 drugs classifies narcotic includ   0.769086\n",
      "10   10537   Phenothiaz   phenothiazine abbreviated ptz organ ... 2580516 doi 10 1016 s1734 1140 12 70726 0 re   0.769061\n",
      "11   06398   Inhalant     inhalants broad range household ind ...  drugs pharmaceutical products used illicitl   0.768755\n",
      "12   06019   Infusion     infusion refers process extracting  ... uid substances directly vein medical purpose   0.768288\n",
      "13   05733   Hershey Ch   hershey–chase experiments series ex ... e dna biomolecule carried genetic informatio   0.768168\n",
      "14   11127   Reflux       reflux suppressant one number medic ... imulants motility stimulants antidopaminergi   0.765571\n",
      "15   09956   Toxin        toxin harmful substance produced wi ... s relates anatomical location effects notabl   0.764637\n",
      "\n",
      "TopK Matchings completed in 3.838026 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"Drug Discovery\", topK=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "936ecd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 3.069359 msec\n",
      "\n",
      "RANK  ID    Wiki Title \t\t\t\t\t\t  MESSAGE \t\t\t\t        CONFIDENCE\n",
      "----|-----|-------------|--------------------------------------------------------------------------------------|---------\n",
      "01   08009   Foreign Ma   malawi former president bakili mulu ... atute international criminal court article 9   0.917747\n",
      "02   07413   Foreign La   foreign relations latvia primary re ...  sweden switzerland thailand turkey venezuel   0.916239\n",
      "03   12761   Foreign Tr   modern trinidad tobago maintains cl ...  represented governor general trinidad tobag   0.913249\n",
      "04   06458   Foreign Ja   jamaica diplomatic relations many n ... coordinating discussions invigorating societ   0.912989\n",
      "05   13585   United Nat   united nations trusteeship council  ... rust territory united states permanent membe   0.911421\n",
      "06   02115   Foreign Ca   cameroon noncontentious low profile ... es well memberships francophonie commonwealt   0.907410\n",
      "07   07465   Foreign Li   lithuania country south eastern sho ... tion security cooperation europe also succes   0.905585\n",
      "08   13582   United Nat   list members united nations securit ... calated world war ii sfn kennedy 2006 p 13–2   0.904387\n",
      "09   11664   Foreign So   foreign relations south africa span ... azi people swazis long ago part swazi kingdo   0.900686\n",
      "10   10804   Foreign Re   two decades preceding republic cong ... dex aspx lang eng embassy canada kinshasa re   0.898769\n",
      "\n",
      "TopK Matchings completed in 3.854686 msec\n"
     ]
    }
   ],
   "source": [
    "Find_TopK_Matchings(query=\"United Nations\", topK=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20374dcd",
   "metadata": {},
   "source": [
    "### <center> End of the Notebook </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969b5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
