/*
 * Copyright 2020 Xilinx, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WANCUNCUANTIES ONCU CONDITIONS OF ANY KIND, either express or
 * implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

USE GRAPH @graph
DROP QUERY read_tg_partition, load_alveo, close_alveo, louvain_alveo

//CREATE DISTRIBUTED QUERY open_alveo() FOR GRAPH @graph {
//   DOUBLE udf_time;
//   String s;
//   INT ret;
//   udf_execute_reset(2);
//   udf_reset_timer(true);
//   // Initialize on all machines
//   Start = {Person.*};
//   A = SELECT v
//       FROM Start:v
//       ACCUM udf_open_alveo(1);
//   udf_time = udf_elapsed_time(true);
//}

CREATE QUERY read_tg_partition(
    SET<STRING> v_type,             // Set of names of vertex types to be considered. Example: ["Person", "Animal"]
    SET<STRING> e_type,             // Set of names of edge types to be considered. Example: ["co-worker", "owner"]
    STRING wt_attr,                 // Name of the edge attribute which has weight of the the edge. Example: "weight"
    STRING louvainId_attr
) {
   ListAccum<UINT> @@nodeAccum;

    MapAccum<UINT,UINT> @@parSize; //nodeid-> parSize
    MapAccum<UINT,UINT> @@offsets; //nodeid -> offsets
    FILE f ("/home2/tigergraph/load_alveo_dev_output.txt");
    SumAccum<UINT> @@total_out_degree;

    
    Start = {v_type};

    nodes = {Person.*};
    dnodes = {dummy_nodes.*};
    @@offsets += (0 -> 0);

    nodeList = SELECT n FROM dnodes:n
            ACCUM @@nodeAccum += udf_xilinx_comdetect_set_node_id(n.NODE_ID) , udf_reset_nextId();
 
    A1 = SELECT n FROM nodes:n
             // Local to each server, louvainid is local increasing id
            ACCUM @@total_out_degree+=n.outdegree()
            POST-ACCUM n.setAttr(louvainId_attr, udf_get_nextId(n.outdegree()))
            ORDER BY n.louvainId;
            
    //print attribute louvainId
    PRINT A1[A1.louvainId];
    PRINT @@total_out_degree;
    
    //print to file
    
    A2 = SELECT n FROM nodes:n
            POST-ACCUM f.println(n, n.louvainId, n.outdegree())
            ORDER BY n.louvainId;
    
    partitionSizeList = SELECT n FROM dnodes:n
            ACCUM @@parSize += (n.NODE_ID -> udf_get_partition_size() );
    
    PRINT @@parSize;

    FOREACH n IN RANGE[0,@@nodeAccum.size()-1] DO
            @@offsets += (n+1 -> @@offsets.get(n)+@@parSize.get(n));
    END;

    offsetList = SELECT n FROM dnodes:n
            ACCUM udf_set_louvain_offset(@@offsets.get(n.NODE_ID));

    PRINT @@offsets;

    Start = SELECT s FROM Start:s-(e_type:e)->:t
            ACCUM udf_set_louvain_edge_list(s.louvainId,t.louvainId,e.getAttr(wt_attr,"FLOAT"),t.outdegree());

}

CREATE DISTRIBUTED QUERY load_alveo(
    SET<STRING> v_types,            // List of vertex types that would participate in Louvain Modularity
    SET<STRING> e_types,            // List of edge types that would participate in Louvain Modularity
    STRING weight_attr,             // Name of the edge attribute that has weight of the edge
    STRING louvainId_attr,
    BOOL tg_partition = TRUE,   // Use the partitions created by TigerGraph if true, else create custom partitions
    BOOL use_saved_partition = TRUE,// If partitions are saved on disk, use them instead of getting from RAM
    STRING graph_file = "",         // Source graph file in .mtx format, needed only if tg_partition = FALSE
    STRING louvain_project = "",    // Provide a project name for saving partitions, used for loading from disk too
    UINT num_partitions = 9,        // Number of partitions as string (e.g. "9"). Default: "auto" for best performance
    UINT num_devices = 1            // Number of devices (alveo cards) to use per server (e.g. "3"). Default; "auto"
) FOR GRAPH @graph
{
    DOUBLE udf_time;
    INT status;
    ListAccum<UINT> @@nodeAccum;

    MapAccum<UINT,UINT> @@parSize; //nodeid-> parSize
    MapAccum<UINT,UINT> @@offsets; //nodeid -> offsets
    SumAccum<UINT> @@total_out_degree;

    //TODO
    //udf_reset_states(1);
    udf_execute_reset(1);
    udf_reset_timer(true);

    // Traverse TigerGraph memory and load partitions on each node
    Start = {v_types};
    nodes = {dummy_nodes.*};
    pnodes = {Person.*};
    @@offsets += (0 -> 0);


    nodeList = SELECT n FROM nodes:n
        ACCUM @@nodeAccum += udf_xilinx_comdetect_set_node_id(n.NODE_ID), udf_reset_nextId();

    IF tg_partition THEN
        A1 = SELECT n FROM pnodes:n
            // Local to each server, louvainid is local increasing id
            ACCUM @@total_out_degree+=n.outdegree()
            POST-ACCUM n.setAttr(louvainId_attr, udf_get_nextId(n.outdegree()))
            ORDER BY n.louvainId;
    
    PRINT A1[A1.louvainId];
    PRINT @@total_out_degree;
        
    partitionSizeList = SELECT n FROM nodes:n
        ACCUM @@parSize += (n.NODE_ID -> udf_get_partition_size() );
    
    PRINT @@parSize;

    FOREACH n IN RANGE[0,@@nodeAccum.size()-1] DO
        @@offsets += (n+1 -> @@offsets.get(n)+@@parSize.get(n));
    END;

    offsetList = SELECT n FROM nodes:n
        ACCUM udf_set_louvain_offset(@@offsets.get(n.NODE_ID));

    PRINT @@offsets;

    Start = SELECT s FROM Start:s-(e_types:e)->:t
        ACCUM udf_set_louvain_edge_list(s.louvainId,t.louvainId,e.getAttr(weight_attr,"FLOAT"),t.outdegree());
            
    parList = SELECT n FROM nodes:n
        ACCUM udf_save_alveo_partition();
        
    /* read_tg_partition(v_types,e_types,weight_attr, louvainId_attr);
        parList = SELECT n FROM nodes:n
            ACCUM udf_save_alveo_partition(); */

    ELSE
        // On each server call the code
        nodelist = SELECT n FROM nodes:n
            ACCUM
            status = udf_create_and_load_alveo_partitions(use_saved_partition, 
                graph_file, louvain_project, nodes.size(), num_partitions, num_devices);
    END;

    udf_time = udf_elapsed_time(true);
    print tg_partition, use_saved_partition, status;

}

CREATE DISTRIBUTED QUERY louvain_alveo(
    SET<STRING> v_type,             // Set of names of vertex types to be considered. Example: ["Person", "Animal"]
    SET<STRING> e_type,             // Set of names of edge types to be considered. Example: ["co-worker", "owner"]
    STRING wt_attr,                 // Name of the edge attribute which has weight of the edge. Example: "weight"
    INT max_iter = 10,              // Maximum number of iterations for moving nodes and evaluating Q for a level
    INT max_level = 10,             // Maximum number of levels or passes of condensing communities and reapplying louvain
    FLOAT tolerence = 0.00001,      // Maximum delta Q that is considered no change
    BOOL intermediateResult = TRUE, // Store intermediate results such as intermediate community in the 'result_file'
    BOOL verbose = FALSE,           // Print debugging messages
    STRING result_attr = "",        // Name of the attribute of a vertex to store the final community Id in
    STRING result_file = "",        // Full path of result file. It must be accessible on each machine in cluster.
    BOOL print_final_Q = TRUE,      // Print final Q value
    BOOL print_all_Q = FALSE)       // Print intermediate Q value
{
    ListAccum<int> @@nodeAccum;
    ListAccum<float> @@modularityAccum;
    DOUBLE udf_time;
    DOUBLE vm_peak, vm_hwm;
    INT ret;

    nodes = {dummy_nodes.*};
    nodeList = SELECT n FROM nodes:n
        ACCUM @@nodeAccum += udf_xilinx_comdetect_set_node_id(n.NODE_ID);

    //nodeList = SELECT n FROM nodes:n
    //    ACCUM @@nodeAccum += udf_xilinx_comdetect_set_num_nodes(1);

    udf_reset_timer(true);
    udf_execute_reset(0);

    // each node will get this UDF call for each vertex, but the actual execution
    // is only run once guarded by a flag. Each node checks if it's master or slave
    // and run the task accordingly.
    //nodelist = SELECT n FROM nodes:n
    //       ACCUM @@executeAlveoStatus += udf_louvain_alveo(
    //       max_iter, max_level, tolerence, intermediateResult,
    //       verbose, result_file, print_final_Q, print_all_Q);
    nodelist = SELECT n FROM nodes:n
           ACCUM @@modularityAccum += udf_louvain_alveo(
           max_iter, max_level, tolerence, intermediateResult,
           verbose, result_file, print_final_Q, print_all_Q);

    udf_time = udf_elapsed_time(true);
    ret = udf_peak_memory_usage(vm_peak, vm_hwm);
    print nodes.size() as NumOfNodes;
    print @@modularityAccum as ModularityQValues;
    print "Xilinx Alveo U50" AS ComputationTechnique;
    print vm_peak as PeakVirtualMemoryInKB;
    print vm_hwm as PeakResidentMemoryInKB;
    print udf_time as ExecTimeInMs;
}

CREATE DISTRIBUTED QUERY close_alveo() for GRAPH @graph {
   DOUBLE udf_time;
   BOOL b;
   udf_reset_timer(true);
   Start = {Person.*};
   A = SELECT v
       FROM Start:v
       ACCUM udf_close_alveo(1);
   udf_time = udf_elapsed_time(true);
   print udf_time, b;
}

install query read_tg_partition , close_alveo, louvain_alveo, load_alveo
