/*
 * Copyright 2020 Xilinx, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WANCUNCUANTIES ONCU CONDITIONS OF ANY KIND, either express or
 * implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

USE GRAPH @graph
DROP QUERY load_alveo, louvain_alveo, close_alveo, tg_partition_phase_1, tg_partition_phase_2, tg_partition_phase_3

//CREATE DISTRIBUTED QUERY open_alveo() FOR GRAPH @graph {
//   DOUBLE udf_time;
//   String s;
//   INT ret;
//   udf_execute_reset(2);
//   udf_reset_timer(true);
//   // Initialize on all machines
//   Start = {Person.*};
//   A = SELECT v
//       FROM Start:v
//       ACCUM udf_open_alveo(1);
//   udf_time = udf_elapsed_time(true);
//}


CREATE DISTRIBUTED QUERY tg_partition_phase_1(
    SET<STRING> v_types,            // List of vertex types that would participate in Louvain Modularity
    SET<STRING> e_types,            // List of edge types that would participate in Louvain Modularity
    STRING weight_attr,             // Name of the edge attribute that has weight of the edge
    STRING louvainId_attr
) FOR GRAPH @graph
{
    //ListAccum<UINT> @@nodeAccum;
    SumAccum<UINT> @@total_out_degree;

    Start = {v_types};
    nodes = {dummy_nodes.*};

    nodeList = SELECT n FROM nodes:n
        ACCUM udf_xilinx_comdetect_set_node_id(n.NODE_ID), udf_reset_nextId();

    A1 = SELECT n FROM Start:n
        // Local to each server, louvainid is local increasing id
        ACCUM @@total_out_degree+=n.outdegree()
        POST-ACCUM n.setAttr(louvainId_attr, udf_get_nextId(n.num, n.outdegree()))
        ORDER BY n.louvainId;
            
    PRINT @@total_out_degree;
}

CREATE DISTRIBUTED QUERY tg_partition_phase_2(
    SET<STRING> v_types,            // List of vertex types that would participate in Louvain Modularity
    SET<STRING> e_types,            // List of edge types that would participate in Louvain Modularity
    STRING weight_attr,             // Name of the edge attribute that has weight of the edge
    STRING louvainId_attr
) FOR GRAPH @graph
{
    SumAccum<UINT> @globalId;
    
    MapAccum<UINT,UINT> @@parSize; //nodeid-> parSize
    MapAccum<UINT,UINT> @@offsets; //nodeid -> offsets
    
    Start = {v_types};
    nodes = {dummy_nodes.*};
    @@offsets += (0 -> 0);
  
    partitionSizeList = SELECT n FROM nodes:n
        ACCUM @@parSize += (n.NODE_ID -> udf_get_partition_size() );
    
    PRINT @@parSize;

    FOREACH n IN RANGE[0,nodes.size()-1] DO
        @@offsets += (n+1 -> @@offsets.get(n)+@@parSize.get(n));
    END;

    offsetList = SELECT n FROM nodes:n
        ACCUM udf_set_louvain_offset(@@offsets.get(n.NODE_ID));

    PRINT @@offsets;
    
    //assign global id
    louvainGlobal = SELECT s FROM Start:s
        ACCUM s.@globalId = udf_get_global_louvain_id(s.getAttr(louvainId_attr,"UINT"))
        POST-ACCUM s.setAttr(louvainId_attr ,s.@globalId);
        //POST-ACCUM s.setAttr(louvainId_attr , udf_get_global_louvain_id(s.getAttr(louvainId_attr,"UINT")));
    
}

CREATE DISTRIBUTED QUERY tg_partition_phase_3(
    SET<STRING> v_types,            // List of vertex types that would participate in Louvain Modularity
    SET<STRING> e_types,            // List of edge types that would participate in Louvain Modularity
    STRING weight_attr,             // Name of the edge attribute that has weight of the edge
    STRING louvainId_attr
) FOR GRAPH @graph 
{
    STRING gName = "@graph" ;
    
    print gName;
    Start = {v_types};
    nodes = {dummy_nodes.*};
    vertexList = SELECT s FROM Start:s-(e_types:e)->:t
        ACCUM udf_set_louvain_edge_list(s.num,t.num, s.louvainId,t.louvainId,e.getAttr(weight_attr,"FLOAT"),t.outdegree());
        
 
    
    nodeList = SELECT n FROM nodes:n 
        ACCUM udf_start_partition(gName, Start.size());
        
    // 2) add_partiton inside udf_save_alveo_partition, save path  = /home2/tigergraph/_xilinx_graph_name_prefix(input file name)    
    
    parList = SELECT n FROM nodes:n
        ACCUM udf_save_alveo_partition();
       
    
    nodeList = SELECT n FROM nodes:n
        ACCUM   udf_finish_partition();
  
}

CREATE DISTRIBUTED QUERY load_alveo(
    SET<STRING> v_types,            // List of vertex types that would participate in Louvain Modularity
    SET<STRING> e_types,            // List of edge types that would participate in Louvain Modularity
    STRING weight_attr,             // Name of the edge attribute that has weight of the edge
    BOOL tg_partition = TRUE,   // Use the partitions created by TigerGraph if true, else create custom partitions
    BOOL use_saved_partition = TRUE,// If partitions are saved on disk, use them instead of getting from RAM
    STRING graph_file = "",         // Source graph file in .mtx format, needed only if tg_partition = FALSE
    STRING louvain_project = "",    // Provide a project name for saving partitions, used for loading from disk too
    UINT num_partitions = 9,        // Number of partitions as string (e.g. "9"). Default: "auto" for best performance
    UINT num_devices = 1            // Number of devices (alveo cards) to use per server (e.g. "3"). Default; "auto"
) FOR GRAPH @graph
{
    DOUBLE udf_time;
    ListAccum<int> @@nodeAccum;
    INT status;

    //TODO
    //udf_reset_states(1);
    udf_execute_reset(1);
    udf_reset_timer(true);

    // Traverse TigerGraph memory and load partitions on each node
    Start = {v_types};
    nodes = {dummy_nodes.*};
    nodeList = SELECT n FROM nodes:n
        ACCUM @@nodeAccum += udf_xilinx_comdetect_set_node_id(n.NODE_ID);

    IF tg_partition THEN
        Start = SELECT v FROM Start:v-(e_types:e)->:t
           ACCUM
           // TODO build the edge list and offsets here from TigerGraph memory
           status = udf_load_alveo(num_partitions, num_devices);
    ELSE
        // On each server call the code
        nodelist = SELECT n FROM nodes:n
           ACCUM
           status = udf_create_and_load_alveo_partitions(use_saved_partition, 
               graph_file, louvain_project, nodes.size(), num_partitions, num_devices);
    END;

    udf_time = udf_elapsed_time(true);
    print tg_partition, use_saved_partition, status;

}

CREATE DISTRIBUTED QUERY louvain_alveo(
    SET<STRING> v_type,             // Set of names of vertex types to be considered. Example: ["Person", "Animal"]
    SET<STRING> e_type,             // Set of names of edge types to be considered. Example: ["co-worker", "owner"]
    STRING wt_attr,                 // Name of the edge attribute which has weight of the edge. Example: "weight"
    INT max_iter = 10,              // Maximum number of iterations for moving nodes and evaluating Q for a level
    INT max_level = 10,             // Maximum number of levels or passes of condensing communities and reapplying louvain
    FLOAT tolerence = 0.00001,      // Maximum delta Q that is considered no change
    BOOL intermediateResult = TRUE, // Store intermediate results such as intermediate community in the 'result_file'
    BOOL verbose = FALSE,           // Print debugging messages
    STRING result_attr = "",        // Name of the attribute of a vertex to store the final community Id in
    STRING result_file = "",        // Full path of result file. It must be accessible on each machine in cluster.
    BOOL print_final_Q = TRUE,      // Print final Q value
    BOOL print_all_Q = FALSE)       // Print intermediate Q value
{
    ListAccum<int> @@nodeAccum;
    ListAccum<float> @@modularityAccum;
    DOUBLE udf_time;
    DOUBLE vm_peak, vm_hwm;
    INT ret;

    nodes = {dummy_nodes.*};
    nodeList = SELECT n FROM nodes:n
        ACCUM @@nodeAccum += udf_xilinx_comdetect_set_node_id(n.NODE_ID);

    //nodeList = SELECT n FROM nodes:n
    //    ACCUM @@nodeAccum += udf_xilinx_comdetect_set_num_nodes(1);

    udf_reset_timer(true);
    udf_execute_reset(0);

    // each node will get this UDF call for each vertex, but the actual execution
    // is only run once guarded by a flag. Each node checks if it's master or slave
    // and run the task accordingly.
    //nodelist = SELECT n FROM nodes:n
    //       ACCUM @@executeAlveoStatus += udf_louvain_alveo(
    //       max_iter, max_level, tolerence, intermediateResult,
    //       verbose, result_file, print_final_Q, print_all_Q);
    nodelist = SELECT n FROM nodes:n
           ACCUM @@modularityAccum += udf_louvain_alveo(
           max_iter, max_level, tolerence, intermediateResult,
           verbose, result_file, print_final_Q, print_all_Q);

    udf_time = udf_elapsed_time(true);
    ret = udf_peak_memory_usage(vm_peak, vm_hwm);
    print nodes.size() as NumOfNodes;
    print @@modularityAccum as ModularityQValues;
    print "Xilinx Alveo U50" AS ComputationTechnique;
    print vm_peak as PeakVirtualMemoryInKB;
    print vm_hwm as PeakResidentMemoryInKB;
    print udf_time as ExecTimeInMs;
}

CREATE DISTRIBUTED QUERY close_alveo() for GRAPH @graph {
   DOUBLE udf_time;
   BOOL b;
   udf_reset_timer(true);
   Start = {Person.*};
   A = SELECT v
       FROM Start:v
       ACCUM udf_close_alveo(1);
   udf_time = udf_elapsed_time(true);
   print udf_time, b;
}

install query close_alveo, louvain_alveo, load_alveo, tg_partition_phase_1, tg_partition_phase_2,tg_partition_phase_3
